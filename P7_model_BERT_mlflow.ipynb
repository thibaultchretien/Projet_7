{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e470e19-bf75-4f6a-99e7-32cfeac294c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "# Importation de Scikit-learn pour les modèles et métriques\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import (accuracy_score, recall_score, f1_score, roc_auc_score, \n",
    "                             confusion_matrix, roc_curve)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Importation des modules de traitement de texte NLTK\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Importation de TensorFlow et Keras pour le Deep Learning\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils, layers, metrics as kmetrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Embedding, LSTM, Bidirectional, \n",
    "                                     TimeDistributed, Flatten, GlobalAveragePooling1D)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Importation de Gensim pour le traitement des modèles Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Importation de XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Définition du chemin des données\n",
    "path_data = '/Users/chretien/OpenClassroom/Openclassroom7/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7636d2-b197-4e18-a289-f32b01339d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", sep=',', encoding='ISO-8859-1', header=None,names=['target', 'id', 'date', 'flag', 'user', 'text'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8f0212-0bce-42e6-9c0c-e0469337a7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>719538</th>\n",
       "      <td>0</td>\n",
       "      <td>Vlad &amp;amp; kevin are gone now  i miss them a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385516</th>\n",
       "      <td>0</td>\n",
       "      <td>Kawasaki asplode. New motor time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116912</th>\n",
       "      <td>0</td>\n",
       "      <td>I really don't want to log onto work tonight b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8582</th>\n",
       "      <td>0</td>\n",
       "      <td>my boyfriend is going out of town for 2 days a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591471</th>\n",
       "      <td>0</td>\n",
       "      <td>@markusbriggz call me ASAP... please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1137915</th>\n",
       "      <td>1</td>\n",
       "      <td>@UncleRUSH I do--and it surely does put you in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330331</th>\n",
       "      <td>1</td>\n",
       "      <td>@LinziMG Hope it goes/is going/has gone well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985869</th>\n",
       "      <td>1</td>\n",
       "      <td>is quite confident and inspired at the mo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072160</th>\n",
       "      <td>1</td>\n",
       "      <td>@bcuban OMG ... the Hitler Techno Bar is right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948292</th>\n",
       "      <td>1</td>\n",
       "      <td>@afeceo Hahaha...My battery died a few days ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           target                                               text\n",
       "0 719538        0  Vlad &amp; kevin are gone now  i miss them a b...\n",
       "  385516        0                 Kawasaki asplode. New motor time. \n",
       "  116912        0  I really don't want to log onto work tonight b...\n",
       "  8582          0  my boyfriend is going out of town for 2 days a...\n",
       "  591471        0              @markusbriggz call me ASAP... please \n",
       "...           ...                                                ...\n",
       "1 1137915       1  @UncleRUSH I do--and it surely does put you in...\n",
       "  1330331       1     @LinziMG Hope it goes/is going/has gone well. \n",
       "  985869        1         is quite confident and inspired at the mo \n",
       "  1072160       1  @bcuban OMG ... the Hitler Techno Bar is right...\n",
       "  948292        1  @afeceo Hahaha...My battery died a few days ag...\n",
       "\n",
       "[1600 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garder uniquement colonnes target et text \n",
    "df = df[['target', 'text']]\n",
    "\n",
    "# Remplacer target 4 par 1\n",
    "df[\"target\"] = df[\"target\"].replace(4, 1)\n",
    "\n",
    "# Sample\n",
    "df_sample = df.groupby('target', as_index=False).apply(lambda x : x.sample(frac=0.001))\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4cf3b8b-4a15-416f-93d3-a823ba9da170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "def tokenizer_fct(sentence) :\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    # print(word_tokens)\n",
    "    return word_tokens\n",
    "\n",
    "\n",
    "# Tokenizer split\n",
    "\n",
    "def tokenizer_split_fct(sentence) :\n",
    "    word_tokens = sentence.split(' ')\n",
    "    # print(word_tokens)\n",
    "    return word_tokens\n",
    "\n",
    "# Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_w = list(set(stopwords.words('english')))\n",
    "\n",
    "def stop_word_filter_fct(list_words) :\n",
    "    filtered_w = [w for w in list_words if not w in stop_w]\n",
    "    # print(filtered_w)    \n",
    "    return filtered_w\n",
    "\n",
    "# lower case et alpha (not \"@\")\n",
    "def lower_alpha_fct(list_words) :\n",
    "    fw = [w.lower() for w in list_words if w.isalpha()]\n",
    "    # print(fw)\n",
    "    return fw\n",
    "\n",
    "# lower case et alpha (not \"@\")\n",
    "def lower_not_user_fct(list_words) :\n",
    "    fw = [w.lower() for w in list_words if not w.startswith(\"@\")]\n",
    "    # print(fw\n",
    "    return fw\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------Lemmatizer-----------------------------------\n",
    "\n",
    "\n",
    "def lemma_fct(list_words) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "    \n",
    "#------------------------------Stemming-----------------------------------\n",
    "\n",
    "\n",
    "def stemma_fct(list_words) :\n",
    "    stemming = PorterStemmer()\n",
    "    stemma_w = [stemming.stem(w) for w in list_words]\n",
    "    return stemma_w\n",
    "\n",
    "\n",
    "#-------------------# Fonction de préparation des tweets----------------------------\n",
    "\n",
    "\n",
    "# Fonction de préparation des questions\n",
    "def transform_text(text) :\n",
    "    word_tokens = tokenizer_split_fct(text)\n",
    "    f_w = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_not_user_fct(f_w)\n",
    "    #lem_w = lemma_fct(lw)\n",
    "    filtered_w = stop_word_filter_fct(lw)\n",
    "    # print(filtered_w)\n",
    "    trans_sentence = ' '.join(filtered_w)\n",
    "    \n",
    "    return trans_sentence\n",
    "\n",
    "\n",
    "# Fonction de préparation des questions\n",
    "def transform_text_lem(text) :\n",
    "    word_tokens = tokenizer_split_fct(text)\n",
    "    f_w = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_not_user_fct(f_w)\n",
    "    lem_w = lemma_fct(lw)\n",
    "    filtered_w = stop_word_filter_fct(lem_w)\n",
    "    # print(filtered_w)\n",
    "    trans_sentence = ' '.join(filtered_w)\n",
    "    \n",
    "    return trans_sentence\n",
    "\n",
    "\n",
    "# Fonction de préparation des questions\n",
    "def transform_text_stemma(text) :\n",
    "    word_tokens = tokenizer_split_fct(text)\n",
    "    f_w = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_not_user_fct(f_w)\n",
    "    stemma_w = stemma_fct(lw)\n",
    "    filtered_w = stop_word_filter_fct(stemma_w)\n",
    "    # print(filtered_w)\n",
    "    trans_sentence = ' '.join(filtered_w)\n",
    "    \n",
    "    return trans_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6f18cd-02b8-4cea-8de7-fe0db58b8239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text_base</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>text_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>719538</th>\n",
       "      <td>0</td>\n",
       "      <td>vlad &amp;amp; kevin gone  miss bunch</td>\n",
       "      <td>vlad &amp;amp; kevin gone  miss bunch</td>\n",
       "      <td>vlad &amp;amp; kevin gone  miss bunch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385516</th>\n",
       "      <td>0</td>\n",
       "      <td>kawasaki asplode. new motor time.</td>\n",
       "      <td>kawasaki asplode. new motor time.</td>\n",
       "      <td>kawasaki asplode. new motor time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116912</th>\n",
       "      <td>0</td>\n",
       "      <td>really want log onto work tonight promised thi...</td>\n",
       "      <td>really want log onto work tonight promised thi...</td>\n",
       "      <td>realli want log onto work tonight promis thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8582</th>\n",
       "      <td>0</td>\n",
       "      <td>boyfriend going town 2 days believe not, i'm f...</td>\n",
       "      <td>boyfriend going town 2 day believe not, i'm fe...</td>\n",
       "      <td>boyfriend go town 2 day believ not, i'm feel l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591471</th>\n",
       "      <td>0</td>\n",
       "      <td>call asap... please</td>\n",
       "      <td>call asap... please</td>\n",
       "      <td>call asap... pleas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1137915</th>\n",
       "      <td>1</td>\n",
       "      <td>do--and surely put whole different stratospher...</td>\n",
       "      <td>do--and surely put whole different stratospher...</td>\n",
       "      <td>do--and sure put whole differ stratosphere.  o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330331</th>\n",
       "      <td>1</td>\n",
       "      <td>hope goes/is going/has gone well.</td>\n",
       "      <td>hope goes/is going/has gone well.</td>\n",
       "      <td>hope goes/i going/ha gone well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985869</th>\n",
       "      <td>1</td>\n",
       "      <td>quite confident inspired mo</td>\n",
       "      <td>quite confident inspired mo</td>\n",
       "      <td>quit confid inspir mo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072160</th>\n",
       "      <td>1</td>\n",
       "      <td>omg ... hitler techno bar right top 7-11!</td>\n",
       "      <td>omg ... hitler techno bar right top 7-11!</td>\n",
       "      <td>omg ... hitler techno bar right top 7-11!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948292</th>\n",
       "      <td>1</td>\n",
       "      <td>hahaha...my battery died days ago signed since...</td>\n",
       "      <td>hahaha...my battery died day ago signed since....</td>\n",
       "      <td>hahaha...mi batteri die day ago sign since. de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           target                                          text_base  \\\n",
       "0 719538        0                  vlad &amp; kevin gone  miss bunch   \n",
       "  385516        0                 kawasaki asplode. new motor time.    \n",
       "  116912        0  really want log onto work tonight promised thi...   \n",
       "  8582          0  boyfriend going town 2 days believe not, i'm f...   \n",
       "  591471        0                               call asap... please    \n",
       "...           ...                                                ...   \n",
       "1 1137915       1  do--and surely put whole different stratospher...   \n",
       "  1330331       1                 hope goes/is going/has gone well.    \n",
       "  985869        1                       quite confident inspired mo    \n",
       "  1072160       1         omg ... hitler techno bar right top 7-11!    \n",
       "  948292        1  hahaha...my battery died days ago signed since...   \n",
       "\n",
       "                                                  text_lemma  \\\n",
       "0 719538                   vlad &amp; kevin gone  miss bunch   \n",
       "  385516                  kawasaki asplode. new motor time.    \n",
       "  116912   really want log onto work tonight promised thi...   \n",
       "  8582     boyfriend going town 2 day believe not, i'm fe...   \n",
       "  591471                                call asap... please    \n",
       "...                                                      ...   \n",
       "1 1137915  do--and surely put whole different stratospher...   \n",
       "  1330331                 hope goes/is going/has gone well.    \n",
       "  985869                        quite confident inspired mo    \n",
       "  1072160         omg ... hitler techno bar right top 7-11!    \n",
       "  948292   hahaha...my battery died day ago signed since....   \n",
       "\n",
       "                                                   text_stem  \n",
       "0 719538                   vlad &amp; kevin gone  miss bunch  \n",
       "  385516                  kawasaki asplode. new motor time.   \n",
       "  116912   realli want log onto work tonight promis thing...  \n",
       "  8582     boyfriend go town 2 day believ not, i'm feel l...  \n",
       "  591471                                 call asap... pleas   \n",
       "...                                                      ...  \n",
       "1 1137915  do--and sure put whole differ stratosphere.  o...  \n",
       "  1330331                   hope goes/i going/ha gone well.   \n",
       "  985869                              quit confid inspir mo   \n",
       "  1072160         omg ... hitler techno bar right top 7-11!   \n",
       "  948292   hahaha...mi batteri die day ago sign since. de...  \n",
       "\n",
       "[1600 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# afficher DataFrame clean \n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "tweets['target'] = df_sample['target']\n",
    "tweets['text_base'] = df_sample['text'].apply(lambda x : transform_text(x))\n",
    "tweets['text_lemma'] = df_sample['text'].apply(lambda x : transform_text_lem(x))\n",
    "tweets['text_stem'] = df_sample['text'].apply(lambda x : transform_text_stemma(x))\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc041f3-149f-49b2-b394-9fda9228b2fc",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac8093f3-d537-428f-a49f-1482253d0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d336bb-b3bc-4be0-ac69-686427c172cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2024/10/29 18:06:25 INFO mlflow.tracking.fluent: Experiment with name 'BERT_Classification_Experiment' does not exist. Creating a new experiment.\n",
      "/var/folders/r0/xwmmd8fx1qng_1s0_ngx42rh0000gn/T/ipykernel_11567/1584924338.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 0.6107588492333889\n",
      "Validation Accuracy: 0.753125, Validation AUC: 0.7500489831106234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.70      0.73       151\n",
      "           1       0.75      0.80      0.77       169\n",
      "\n",
      "    accuracy                           0.75       320\n",
      "   macro avg       0.75      0.75      0.75       320\n",
      "weighted avg       0.75      0.75      0.75       320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r0/xwmmd8fx1qng_1s0_ngx42rh0000gn/T/ipykernel_11567/1584924338.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "Train Loss: 0.36632076408714054\n",
      "Validation Accuracy: 0.76875, Validation AUC: 0.7676633096908188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.75      0.75       151\n",
      "           1       0.78      0.79      0.78       169\n",
      "\n",
      "    accuracy                           0.77       320\n",
      "   macro avg       0.77      0.77      0.77       320\n",
      "weighted avg       0.77      0.77      0.77       320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r0/xwmmd8fx1qng_1s0_ngx42rh0000gn/T/ipykernel_11567/1584924338.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "Train Loss: 0.15257935313275084\n",
      "Validation Accuracy: 0.68125, Validation AUC: 0.6696579019554059\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.46      0.58       151\n",
      "           1       0.65      0.88      0.74       169\n",
      "\n",
      "    accuracy                           0.68       320\n",
      "   macro avg       0.71      0.67      0.66       320\n",
      "weighted avg       0.70      0.68      0.67       320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete and model logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Chargement des données et division en train/validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(tweets['text_base'], tweets['target'], test_size=0.2)\n",
    "\n",
    "# 2. Chargement du tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 3. Tokenization des données\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "train_encodings = tokenize_texts(train_texts.tolist())\n",
    "val_encodings = tokenize_texts(val_texts.tolist())\n",
    "\n",
    "# 4. Création d'une classe Dataset pour PyTorch\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 5. Création des DataLoaders\n",
    "train_dataset = TextDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = TextDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# 6. Chargement du modèle BERT pour la classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 pour classification binaire\n",
    "\n",
    "# 7. Configuration de l'optimiseur\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Configuration de MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5003\")\n",
    "mlflow.set_experiment(\"BERT_Classification_Experiment\")\n",
    "\n",
    "# 8. Fonction d'entraînement\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 9. Fonction de validation\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_preds += (preds == batch['labels']).sum().item()\n",
    "            total_preds += len(preds)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    accuracy = correct_preds / total_preds\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    return accuracy, auc, all_preds, all_labels\n",
    "\n",
    "# 10. Entraînement du modèle\n",
    "epochs = 3\n",
    "with mlflow.start_run(run_name=\"BERT_Training_Run\"):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_loader)\n",
    "        val_acc, val_auc, val_preds, val_labels = validate(model, val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "        print(f\"Validation Accuracy: {val_acc}, Validation AUC: {val_auc}\")\n",
    "        print(classification_report(val_labels, val_preds))\n",
    "\n",
    "        # Loguer uniquement les métriques de validation dans MLflow\n",
    "        mlflow.log_metric(\"Validation Accuracy\", val_acc)\n",
    "        mlflow.log_metric(\"Validation AUC\", val_auc)\n",
    "\n",
    "    # Enregistrer le modèle avec MLflow\n",
    "    mlflow.pytorch.log_model(model, \"bert_model\")\n",
    "\n",
    "print(\"Training complete and model logged to MLflow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b55f73-c634-4ec6-8b11-5690ec53ae1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2cbead-8328-43a9-9be0-a2caaf1c169b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b68f3-9e44-4105-90c1-ddd3529f88dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f6a98-dfaf-43bc-bd48-99176e1f58ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8fcfe7-68b7-4624-b606-bc4324e2cebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fe2f2-3e83-4d1b-b57c-0e88c04d7be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8369199-fc2c-4a3b-b90c-675e01086c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ce06d-0535-47f0-85fe-c1082247e610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542d2f9-2aa4-42a4-913f-8a89fb773504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52706e27-f618-4388-a92f-2613f82c71e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137c5a4-0d5b-4298-9524-5a3edbd935dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7d66b54d-6da6-480e-8b89-4b5b03fd137e",
   "metadata": {},
   "source": [
    "# 1. Chargement des données et division en train/validation\n",
    "# Supposons que vos données soient dans un DataFrame `df` avec les colonnes 'text' et 'target'\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(tweets['text_base'], tweets['target'], test_size=0.2)\n",
    "\n",
    "# 2. Chargement du tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "001a0918-45da-412e-9ee3-1ad3f217f7c1",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Tokenization des données\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "train_encodings = tokenize_texts(train_texts.tolist())\n",
    "val_encodings = tokenize_texts(val_texts.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed88d967-21fa-4d43-890a-ac7340ec4117",
   "metadata": {},
   "source": [
    "# 4. Création d'une classe Dataset pour PyTorch\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb2be5be-ef68-4cee-bc9f-121aa0590b28",
   "metadata": {},
   "source": [
    "# 5. Création des DataLoaders\n",
    "train_dataset = TextDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = TextDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e32c9128-a4d6-454e-8885-58e8887f9a4b",
   "metadata": {},
   "source": [
    "# 6. Chargement du modèle BERT pour la classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 pour classification binaire\n",
    "\n",
    "\n",
    "# 7. Configuration de l'optimiseur\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fb2e725-9413-4b0a-a88c-aa5ea1f7a1f4",
   "metadata": {},
   "source": [
    "# 8. Fonction d'entraînement\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calcul des prédictions\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_preds += (preds == batch['labels']).sum().item()\n",
    "        total_preds += len(preds)\n",
    "\n",
    "    return total_loss / len(dataloader), correct_preds / total_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2de4c182-8059-4766-86c5-f5ba2daa9f97",
   "metadata": {},
   "source": [
    "# 9. Fonction de validation\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_preds += (preds == batch['labels']).sum().item()\n",
    "            total_preds += len(preds)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    accuracy = correct_preds / total_preds\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    return accuracy, auc, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ae2ffa3-2eab-4933-abfb-d012dba7d5ac",
   "metadata": {},
   "source": [
    "# 10. Entraînement du modèle\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader)\n",
    "    val_acc, val_auc, val_preds, val_labels = validate(model, val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_acc}\")\n",
    "    print(f\"Validation Accuracy: {val_acc}, Validation AUC: {val_auc}\")\n",
    "    print(classification_report(val_labels, val_preds))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f08b8d15-981f-4913-bfb8-9115995a967e",
   "metadata": {},
   "source": [
    "\n",
    "# 11. Sauvegarder le modèle entraîné\n",
    "model.save_pretrained('./bert_model')\n",
    "tokenizer.save_pretrained('./bert_tokenizer')\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc078d9a-5328-4224-a368-388bdb416d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
